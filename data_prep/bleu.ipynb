{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c02273-1424-4280-b1e3-ecbf4b44fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "import random\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740423f6-e318-4686-b332-4411b798ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 53\n",
    "\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "#pd.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2815495a-b3fe-44f9-ba80-18ef373bb4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(pred_list, ref_list):\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    return bleu.compute(predictions=pred_list, references=ref_list, max_order=4)['bleu']\n",
    "    # print(bleu.compute(predictions=pred_list, references=ref_list, max_order=4)['bleu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc21d31-f0a8-47a4-b449-1b95f5801c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667abbda-b6e9-43c1-94b5-d0acd87c5dcb",
   "metadata": {},
   "source": [
    "### Translation and Back-translation in MSF-BT process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3cd040-57f4-49d6-b772-cf191900d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = 'ur' # all the langs one by one\n",
    "train_df = pd.read_csv(data_path+lang1+'_train.csv')\n",
    "dev_df = pd.read_csv(data_path+lang1+'_dev.csv')\n",
    "test_df = pd.read_csv(data_path+lang1+'_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb60e241-a268-4622-a1ea-9be135bda2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang2 = 'en' #For lang1 = 'en', this will 'hi' otherwise it is 'en', but for back-translation lang1 and 2 will be same\n",
    "en_train_df = pd.read_csv(data_path+lang2+'_train.csv')\n",
    "en_dev_df = pd.read_csv(data_path+lang2+'_dev.csv')\n",
    "en_test_df = pd.read_csv(data_path+lang2+'_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad825a8-80ea-4506-8d7e-476a8efb7ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For translation\n",
    "pred_pos_column = 'POSITIVE_TR'\n",
    "pred_neg_column = 'NEGATIVE_TR'\n",
    "\n",
    "# For back-translation\n",
    "# pred_pos_column = 'POSITIVE_TR_TR'\n",
    "# pred_neg_column = 'NEGATIVE_TR_TR'\n",
    "\n",
    "ref_pos_column = 'POSITIVE'\n",
    "ref_neg_column = 'NEGATIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aa6ca7-eea2-4c3e-9199-8c488b0eb407",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [train_df, dev_df, test_df]\n",
    "refs = [en_train_df, en_dev_df, en_test_df]\n",
    "\n",
    "pred_data = []\n",
    "ref_data = []\n",
    "for df, ref in zip(dfs, refs):\n",
    "    pred_data.extend(df[pred_pos_column].tolist() + df[pred_neg_column].tolist())\n",
    "    ref_data.extend(ref[ref_pos_column].tolist() + ref[ref_neg_column].tolist())\n",
    "\n",
    "print(f\"BLEU Score for {lang1}:\")\n",
    "calculate_bleu(pred_data, ref_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7ac72d-a043-4b13-ab95-2d72d9cd4efc",
   "metadata": {},
   "source": [
    "### Translations\n",
    "\n",
    "#### BLEU Score for en:\n",
    "0.2068746173173193\n",
    "\n",
    "#### BLEU Score for hi:\n",
    "0.2608698044490646\n",
    "\n",
    "#### BLEU Score for mag:\n",
    "0.18119490941562166\n",
    "\n",
    "#### BLEU Score for ml:\n",
    "0.32859407934998675\n",
    "\n",
    "#### BLEU Score for mr:\n",
    "0.32360376399016005\n",
    "\n",
    "#### BLEU Score for or:\n",
    "0.33073082902902456\n",
    "\n",
    "#### BLEU Score for pa:\n",
    "0.3455499437168114\n",
    "\n",
    "#### BLEU Score for te:\n",
    "0.24689777485272318\n",
    "\n",
    "#### BLEU Score for ur:\n",
    "0.3841462156590448\n",
    "\n",
    "\n",
    "### Back-translations\n",
    "\n",
    "#### BLEU Score for en:\n",
    "0.42631642755111215\n",
    "\n",
    "#### BLEU Score for hi:\n",
    "0.29972691999559\n",
    "\n",
    "#### BLEU Score for mag:\n",
    "0.07971240072682187\n",
    "\n",
    "#### BLEU Score for ml:\n",
    "0.20690417918405446\n",
    "\n",
    "#### BLEU Score for mr:\n",
    "0.2732078802698464\n",
    "\n",
    "#### BLEU Score for or:\n",
    "0.21772176469405635\n",
    "\n",
    "#### BLEU Score for pa:\n",
    "0.38206792153290997\n",
    "\n",
    "#### BLEU Score for te:\n",
    "0.14217740939303822\n",
    "\n",
    "#### BLEU Score for ur:\n",
    "0.4098485427039675\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a909b0-28a7-4f12-99a1-40b3f41735be",
   "metadata": {},
   "source": [
    "### Crosslingual experiments in En-IP-TR-Train experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03696c57-cfa2-4139-8470-c10ef5db0594",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train_df = pd.read_csv(data_path+'en_train.csv')\n",
    "en_dev_df = pd.read_csv(data_path+'en_dev.csv')\n",
    "en_test_df = pd.read_csv(data_path+'en_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faada2a-b757-4eab-9ade-ff9efa64804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = {\n",
    "    'hin': 'Deva',  # hin -> hi\n",
    "    'mag': 'Deva',  # mag -> mag\n",
    "    'mal': 'Mlym',   # mal -> ml\n",
    "    'mar': 'Deva',  # mar -> mr\n",
    "    'ory': 'Orya',  # ory -> or\n",
    "    'pan': 'Guru',  # pan -> pa\n",
    "    'tel': 'Telu',  # tel -> te\n",
    "    'urd': 'Arab'   # urd -> ur\n",
    "}\n",
    "\n",
    "ref_languages = ['hi', 'mag', 'ml', 'mr', 'or', 'pa', 'te', 'ur']\n",
    "\n",
    "for index, (lang, suffix) in enumerate(list(languages.items())):\n",
    "    positive_col = f\"POSITIVE_{lang}_{suffix}\"\n",
    "    negative_col = f\"NEGATIVE_{lang}_{suffix}\"\n",
    "    pred_texts_train = en_train_df[positive_col].tolist() + en_train_df[negative_col].tolist()\n",
    "    pred_texts_dev = en_dev_df[positive_col].tolist() + en_dev_df[negative_col].to_list()\n",
    "    pred_texts_test = en_test_df[positive_col].tolist() + en_test_df[negative_col].tolist()\n",
    "    pred_texts = pred_texts_train + pred_texts_dev + pred_texts_test\n",
    "\n",
    "\n",
    "    train_df = pd.read_csv(f\"{data_path}{ref_languages[index]}_train.csv\")\n",
    "    dev_df = pd.read_csv(f\"{data_path}{ref_languages[index]}_dev.csv\")\n",
    "    test_df = pd.read_csv(f\"{data_path}{ref_languages[index]}_test.csv\")\n",
    "    \n",
    "    trg_texts_train_positive = train_df['POSITIVE'].tolist()\n",
    "    trg_texts_train_negative = train_df['NEGATIVE'].tolist()\n",
    "    \n",
    "    trg_texts_dev_positive = dev_df['POSITIVE'].tolist()\n",
    "    trg_texts_dev_negative = dev_df['NEGATIVE'].tolist()\n",
    "    \n",
    "    trg_texts_test_positive = test_df['POSITIVE'].tolist()\n",
    "    trg_texts_test_negative = test_df['NEGATIVE'].tolist()\n",
    "    \n",
    "    trg_texts_train = trg_texts_train_positive + trg_texts_train_negative\n",
    "    trg_texts_dev = trg_texts_dev_positive + trg_texts_dev_negative\n",
    "    trg_texts_test = trg_texts_test_positive + trg_texts_test_negative\n",
    "    \n",
    "    trg_texts = trg_texts_train + trg_texts_dev + trg_texts_test\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"BLEU Score for {ref_languages[index]}:\")\n",
    "    calculate_bleu(pred_texts, trg_texts)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d649338-8362-40ba-a760-03220d02a9f7",
   "metadata": {},
   "source": [
    "### En-OP-TR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782260f4-d576-441d-9916-25e71657442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "languages = ['hi', 'mag', 'ml', 'mr', 'or', 'pa', 'te', 'ur']\n",
    "\n",
    "directory = '../output/'\n",
    "\n",
    "# def calculate_bleu(reference, hypothesis):\n",
    "#     smoothie = SmoothingFunction().method4\n",
    "#     return sentence_bleu([reference.split()], hypothesis.split(), smoothing_function=smoothie)\n",
    "\n",
    "bleu_scores = {}\n",
    "\n",
    "for lang in languages:\n",
    "    pos_to_neg_source = pd.read_csv(os.path.join(directory, f'pos_to_neg-{lang}-en_op_tr.csv'))\n",
    "    neg_to_pos_source = pd.read_csv(os.path.join(directory, f'neg_to_pos-{lang}-en_op_tr.csv'))\n",
    "    \n",
    "    source_preds = pos_to_neg_source['pred'].tolist() + neg_to_pos_source['pred'].tolist()\n",
    "\n",
    "    pos_to_neg_target = pd.read_csv(os.path.join(directory, f'pos_to_neg-{lang}-parallel.csv'))\n",
    "    neg_to_pos_target = pd.read_csv(os.path.join(directory, f'neg_to_pos-{lang}-parallel.csv'))\n",
    "    \n",
    "    target_preds = pos_to_neg_target['pred'].tolist() + neg_to_pos_target['pred'].tolist()\n",
    "\n",
    "    if len(source_preds) == len(target_preds):\n",
    "        bleu_score = calculate_bleu(source_preds, target_preds) #sum(calculate_bleu(ref, hyp) for ref, hyp in zip(source_preds, target_preds)) / len(source_preds)\n",
    "        bleu_scores[lang] = bleu_score*100\n",
    "    else:\n",
    "        print(f\"Mismatch in number of predictions for {lang}. Skipping BLEU calculation.\")\n",
    "\n",
    "for lang, score in bleu_scores.items():\n",
    "    print(f\"{lang}: BLEU Score = {score:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b342ec-8ece-4392-813a-d1bed99b3195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
